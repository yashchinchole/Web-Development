<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YASH 1</title>
</head>

<body>
    <h1>ChatGPT</h1>
    <hr>
    <p><strong>ChatGPT</strong> <sup>[a]</sup> is an artificial intelligence chatbot developed by <b>OpenAI</b> and
        launched in
        November 2022. It is built on top of OpenAI's GPT-3 family of large language models and has been fine-tuned (an
        approach to transfer learning) using both supervised and reinforcement learning techniques.
        <br>
        ChatGPT was launched as a prototype on November 30, 2022, and quickly garnered attention for its detailed
        responses and articulate answers across many domains of knowledge.[3] Its uneven factual accuracy, however, has
        been identified as a significant drawback.[4] Following the release of ChatGPT, OpenAI's valuation was estimated
        at US$29 billion in 2023.[5]
        <br>
        <i>GPT-4</i> , the newest OpenAI model, was released on March 14, 2023, and is available for ChatGPT Plus users.
    </p>

    <!-- <img width="200"
        src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/ChatGPT_logo.svg/180px-ChatGPT_logo.svg.png"
        alt="Loading Error">

    <img width="400" src="src/i1.jpg" alt="Loading Error">

    <img width="400" srcset="src/i2.jpeg" src="src/i.jpeg" alt="Loading Error"> -->

    <!-- <img width="100%" src="src/i3.jpg" alt=""> -->

    <!-- <img width="400" srcset="src/i2.jpeg 1000w,
    src/i3.jpg 700w,
    src/i4.jpg 500w" sizes="(max-width: 500px) 500px,
    (max-width: 700px) 700px,
    1000px" src="src/i1.jpg" alt=""> -->

    <h2>Training</h2>
    <hr>
    <p>ChatGPT is a member of the generative pre-trained transformer (GPT) family of language models. It was fine-tuned
        (an approach to transfer learning[6]) over an improved version of OpenAI's GPT-3 known as "GPT 3.5".[7] The
        fine-tuning process leveraged both supervised learning as well as reinforcement learning in a process called
        reinforcement learning from human feedback (RLHF).[8][9] Both approaches used human trainers to improve the
        model's performance. In the case of supervised learning, the model was provided with conversations in which the
        trainers played both sides: the user and the AI assistant. In the reinforcement learning step, human trainers
        first ranked responses that the model had created in a previous conversation.[10] These rankings were used to
        create 'reward models' that the model was further fine-tuned on using several iterations of Proximal Policy
        Optimization (PPO).[8][11] Proximal Policy Optimization algorithms present a cost-effective benefit to trust
        region policy optimization algorithms; they negate many of the computationally expensive operations with faster
        performance.[12][13] The models were trained in collaboration with Microsoft on their Azure supercomputing
        infrastructure, using Nvidia GPUs, "supercomputer developed for OpenAI is a single system with more than 285,000
        CPU cores, 10,000 GPUs and 400 gigabits per second of network connectivity for each GPU server".[14]
        <br>
        In addition, OpenAI continues to gather data from ChatGPT users that could be used to further train and
        fine-tune ChatGPT. Users can upvote or downvote responses they receive from ChatGPT and fill out a text field
        with additional feedback.[15][16
    </p>
</body>

</html>